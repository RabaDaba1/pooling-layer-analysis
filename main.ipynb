{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = Path('.')\n",
    "DATA_DIR = ROOT_DIR / 'data'\n",
    "REPORTS_DIR = ROOT_DIR / 'reports'\n",
    "MODELS_DIR = REPORTS_DIR / 'models'\n",
    "RESULTS_DIR = REPORTS_DIR / 'results'\n",
    "RUNS_DIR = REPORTS_DIR / 'runs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "     transforms.RandomHorizontalFlip(p=0.5),\n",
    "     transforms.RandomVerticalFlip(p=0.5)]\n",
    ")\n",
    "val_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "     transforms.RandomHorizontalFlip(p=0.5),\n",
    "     transforms.RandomVerticalFlip(p=0.5)]\n",
    ")\n",
    "test_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proportion = 0.9\n",
    "num_train = 50000\n",
    "\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(train_proportion * num_train))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_idx, val_idx = indices[:split], indices[split:]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = datasets.CIFAR10(root=DATA_DIR, train=True, \n",
    "                                 download=True, transform=train_transform)\n",
    "\n",
    "val_dataset = datasets.CIFAR10(root=DATA_DIR, train=True, \n",
    "                               download=True, transform=val_transform)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root=DATA_DIR, train=False,\n",
    "                                download=True, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, \n",
    "                          sampler=train_sampler, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, \n",
    "                        sampler=val_sampler, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                         num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OWALayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = bias  # Separate flag\n",
    "        self.weights = nn.Parameter(torch.empty((out_features, in_features)))\n",
    "        if self.use_bias:\n",
    "            self.bias = nn.Parameter(torch.empty(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.init_parameters()\n",
    "        \n",
    "    def init_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weights.size(1))\n",
    "        nn.init.uniform_(self.weights, -stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            nn.init.uniform_(self.bias, -stdv, stdv)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # Specify the dimension to sort along, typically the feature dimension\n",
    "        x_sorted, _ = torch.sort(x, dim=1, descending=True)\n",
    "        normalized_weights = F.softmax(self.weights, dim=1)\n",
    "        output = torch.matmul(x_sorted, normalized_weights.t())\n",
    "        if self.bias is not None:\n",
    "            output += self.bias\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChoquetLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(ChoquetLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = bias\n",
    "\n",
    "        self.v = nn.Parameter(torch.empty(out_features, in_features))\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = nn.Parameter(torch.empty(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.init_parameters()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.in_features)\n",
    "        nn.init.uniform_(self.v, -stdv, stdv)\n",
    "        if self.use_bias:\n",
    "            nn.init.uniform_(self.bias, -stdv, stdv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() != 2:\n",
    "            raise ValueError(\"Input tensor must be 2D (batch_size, in_features)\")\n",
    "\n",
    "        x_sorted, _ = torch.sort(x, dim=1, descending=True)\n",
    "        v_padded = F.pad(self.v, (0, 1), \"constant\", 0)\n",
    "        delta_v = v_padded[:, :-1] - v_padded[:, 1:]\n",
    "        x_sorted_expanded = x_sorted.unsqueeze(1)  # (batch_size, 1, in_features)\n",
    "        delta_v_expanded = delta_v.unsqueeze(0)  # (1, out_features, in_features)\n",
    "        output = torch.sum(x_sorted_expanded * delta_v_expanded, dim=2)\n",
    "\n",
    "        if self.use_bias:\n",
    "            output += self.bias\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: finish implementing this\n",
    "class AggPoolingLayer(nn.Module):\n",
    "    def __init__(self, kernel_size, stride, padding):\n",
    "        super(AggPoolingLayer, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.pad(\n",
    "            x,\n",
    "            (self.padding, self.padding, self.padding, self.padding),\n",
    "            mode=\"constant\",\n",
    "            value=0,\n",
    "        )\n",
    "        x_shape = x.shape\n",
    "        x = x.unfold(2, self.kernel_size, self.stride).unfold(\n",
    "            3, self.kernel_size, self.stride\n",
    "        )  # (batch_size, in_channels, num_patches_height, num_patches_width, kernel_size, kernel_size)\n",
    "        x = x.reshape(\n",
    "            x_shape[0],\n",
    "            x_shape[1],\n",
    "            x_shape[2]//self.stride,\n",
    "            x_shape[3]//self.stride,\n",
    "            self.kernel_size**2,\n",
    "        )  # (batch_size, in_channels, img_height, img_width, kernel_size*kernel_size) for each pixel there are kernel_size**2 values\n",
    "        x, _ = x.max(dim=4)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNetModel(nn.Module):\n",
    "\n",
    "    def __init__(self, conv_filters=[64, 64], linear_sizes=[384, 192], num_classes=10):\n",
    "        super(LeNetModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, conv_filters[0], [2,2], [1,1])\n",
    "        self.pool1 = nn.MaxPool2d([2,2], [2,2])\n",
    "        self.conv2 = nn.Conv2d(conv_filters[0], conv_filters[1], [2,2], [1,1])\n",
    "        self.pool2 = nn.MaxPool2d([2,2], [2,2])\n",
    "        self.fc1 = nn.Linear(conv_filters[1]*7*7, linear_sizes[0])       \n",
    "        self.fc2 = nn.Linear(linear_sizes[0], linear_sizes[1])\n",
    "        self.fc3 = nn.Linear(linear_sizes[1], num_classes)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir=RUNS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, val_loader=None, num_epochs=20, device='cuda'):\n",
    "    train_acc = []  \n",
    "    train_loss = []\n",
    "    if val_loader is not None:\n",
    "        val_acc = []\n",
    "        val_loss = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        count_evaluated = 0\n",
    "        count_correct = 0\n",
    "        \n",
    "        for batch_idx, data in enumerate(train_loader, 0):    \n",
    "            model.train()  \n",
    "            inputs, labels = data[0].to(device), data[1].to(device)  \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            count_evaluated += inputs.shape[0]\n",
    "            count_correct += torch.sum(labels == torch.max(outputs, dim=1)[1])\n",
    "            \n",
    "        print('Training: [%d, %5d] loss: %.3f' % (epoch + 1, batch_idx + 1, running_loss / (batch_idx+1)))\n",
    "        \n",
    "        train_loss.append(running_loss / (batch_idx+1))\n",
    "        train_acc.append(float(count_correct) / count_evaluated)\n",
    "        \n",
    "        if val_loader is not None:\n",
    "            running_loss_val = 0.0\n",
    "            count_evaluated = 0\n",
    "            count_correct = 0\n",
    "            model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for val_batch_idx, data_val in enumerate(val_loader, 0):\n",
    "                    inputs_val, labels_val = data_val[0].to(device), data_val[1].to(device)\n",
    "                    outputs_val = model(inputs_val)\n",
    "                    loss = criterion(outputs_val, labels_val)\n",
    "                    running_loss_val += loss.item()\n",
    "                    count_evaluated += inputs_val.shape[0]\n",
    "                    count_correct += torch.sum(labels_val == torch.max(outputs_val, dim=1)[1])\n",
    "                    \n",
    "                val_loss.append(running_loss_val / (val_batch_idx + 1))\n",
    "                acc_val = float(count_correct) / count_evaluated\n",
    "                \n",
    "                print('Validation: epoch %d - acc: %.3f' %\n",
    "                            (epoch + 1, acc_val))\n",
    "                val_acc.append(acc_val)\n",
    "                \n",
    "        # Tensorboard\n",
    "        writer.add_scalar('Loss/Validation', val_loss[-1], global_step=epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc[-1], global_step=epoch)\n",
    "        writer.add_scalar('Loss/Train', train_loss[-1], global_step=epoch)\n",
    "        writer.add_scalar('Accuracy/Train', train_acc[-1], global_step=epoch)\n",
    "        for name, param in model.named_parameters():\n",
    "            writer.add_histogram(f\"Parameters/{name}\", param, epoch)\n",
    "            if param.grad is not None:\n",
    "                writer.add_histogram(f\"Gradients/{name}\", param.grad, epoch)\n",
    "                \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, criterion, device='cuda'):\n",
    "    with torch.no_grad():\n",
    "        number_samples = 0\n",
    "        number_correct = 0\n",
    "        running_loss_test = 0.0\n",
    "        for test_batch_idx, data_test in enumerate(test_loader, 0):\n",
    "            inputs_test, labels_test = data_test[0].to(device), data_test[1].long().to(device)\n",
    "            outputs_test = model(inputs_test)\n",
    "            loss = criterion(outputs_test, labels_test)\n",
    "            running_loss_test += loss.cpu().numpy()\n",
    "            \n",
    "            _, outputs_class = torch.max(outputs_test, dim=1)\n",
    "            number_correct += torch.sum(outputs_class == labels_test).cpu().numpy()\n",
    "            number_samples += len(labels_test)\n",
    "            \n",
    "        acc_test = number_correct / number_samples\n",
    "        \n",
    "        print('Test - Accuracy: %.3f' % acc_test)\n",
    "        print('Test - CrossEntropy: %.3f' % (running_loss_test / (test_batch_idx+1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
