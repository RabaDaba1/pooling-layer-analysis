{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = Path('.')\n",
    "DATA_DIR = ROOT_DIR / 'data'\n",
    "REPORTS_DIR = ROOT_DIR / 'reports'\n",
    "MODELS_DIR = REPORTS_DIR / 'models'\n",
    "RESULTS_DIR = REPORTS_DIR / 'results'\n",
    "RUNS_DIR = REPORTS_DIR / 'runs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "     transforms.RandomHorizontalFlip(p=0.5),\n",
    "     transforms.RandomVerticalFlip(p=0.5)]\n",
    ")\n",
    "val_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "     transforms.RandomHorizontalFlip(p=0.5),\n",
    "     transforms.RandomVerticalFlip(p=0.5)]\n",
    ")\n",
    "test_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proportion = 0.9\n",
    "num_train = 50000\n",
    "\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(train_proportion * num_train))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_idx, val_idx = indices[:split], indices[split:]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = datasets.CIFAR10(root=DATA_DIR, train=True, \n",
    "                                 download=True, transform=train_transform)\n",
    "\n",
    "val_dataset = datasets.CIFAR10(root=DATA_DIR, train=True, \n",
    "                               download=True, transform=val_transform)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root=DATA_DIR, train=False,\n",
    "                                download=True, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, \n",
    "                          sampler=train_sampler, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, \n",
    "                        sampler=val_sampler, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                         num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arithmetic_mean(X, dim, keepdim):\n",
    "    return torch.mean(X, dim, keepdim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimum(X, dim, keepdim): \n",
    "    return torch.min(X, dim, keepdim).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product(X, dim, keepdim)\n",
    "    return torch.prod(X, dim=dim, keepdim=keepdim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_norm_lukasiewicz(X, dim, keepdim):\n",
    "    sum_X = torch.sum(X, dim=dim, keepdim=keepdim) - 1\n",
    "    return torch.max(sum_X, torch.tensor(0, device=X.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hamacker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum(X, dim, keepdim): \n",
    "    return torch.max(X, dim, keepdim).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_conorm_lukasiewicz(X, dim, keepdim):\n",
    "    sum_X = torch.sum(X, dim=dim, keepdim=keepdim)\n",
    "    return torch.min(sum_X, torch.tensor(1.0, device=X.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_conorm_hamacker(X, dim, keepdim):\n",
    "    product_X = torch.prod(X, dim=dim, keepdim=keepdim)\n",
    "    \n",
    "    sum_X = torch.sum(X, dim=dim, keepdim=keepdim)\n",
    "    \n",
    "    numerator = 2 * product_X - sum_X\n",
    "    denominator = product_X - 1\n",
    "    \n",
    "    result = torch.where(\n",
    "        product_X == 1,\n",
    "        torch.tensor(1.0),\n",
    "        numerator / (denominator + 1e-10)  # Adding a small value for numerical stability.\n",
    "    )\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u_min_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u_L_L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggPoolingLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, kernel_size, stride, padding= [0,0,0,0], function, dim = -1, keepdim = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Una tupla de 2 elementos con los tamaños [𝑘1,𝑘2] de cada ventana a tratar\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        # Tupla de 2 elementos que indican el número de elementos (en filas y columnas) que \n",
    "        # deben saltarse tras reducir cada ventana, hasta encontrar la siguiente a tratar.\n",
    "        self.stride = stride\n",
    "        \n",
    "        # Tupla de 4 elementos de la forma [𝑝𝑎𝑑_𝑙𝑒𝑓𝑡,𝑝𝑎𝑑_𝑟𝑖𝑔ℎ𝑡,𝑝𝑎𝑑_𝑢𝑝,𝑝𝑎𝑑_𝑑𝑜𝑤𝑛] que indica el \n",
    "        # número de nuevas filas o columnas a añadir a la entrada, previo a aplicar la agregación.\n",
    "        self.padding = padding\n",
    "        \n",
    "        # Define function and characteristics\n",
    "        self.function = function\n",
    "        self.dim = dim\n",
    "        self.keepdim = keepdim\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \n",
    "        # Normalize\n",
    "        maximum = torch.max(X)\n",
    "        minimum = torch.min(X)\n",
    "        X = (X-minimum)/(maximum-minimum)\n",
    "        \n",
    "        # Añadir columnas/filas según padding\n",
    "        X_pad = F.pad(X, pad=self.padding, mode='constant', value=0)\n",
    "        \n",
    "        # Vamos extrayendo las ventanas a agregar y colocándolas en filas\n",
    "        X_aux = X_pad.unfold(2, size=self.kernel_size[0], step=self.stride[0]).unfold(3, size=self.kernel_size[1], step=self.stride[1])\n",
    "        \n",
    "        # Ponemos el formato correcto\n",
    "        X_aux = X_aux.reshape([X_aux.shape[0], X_aux.shape[1], X_aux.shape[2], X_aux.shape[3], X_aux.shape[4] * X_aux.shape[5]]) \n",
    "        \n",
    "        # Agg Func\n",
    "        Y_temp = self.function(X_aux, dim = self.dim, keepdim = self.keepdim)\n",
    "        \n",
    "        # Denormalize \n",
    "        Y = minimum + (maximum-minimum) * Y_temp\n",
    "        \n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OWALayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = bias  # Separate flag\n",
    "        self.weights = nn.Parameter(torch.empty((out_features, in_features)))\n",
    "        if self.use_bias:\n",
    "            self.bias = nn.Parameter(torch.empty(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.init_parameters()\n",
    "        \n",
    "    def init_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weights.size(1))\n",
    "        nn.init.uniform_(self.weights, -stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            nn.init.uniform_(self.bias, -stdv, stdv)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # Specify the dimension to sort along, typically the feature dimension\n",
    "        x_sorted, _ = torch.sort(x, dim=1, descending=True)\n",
    "        normalized_weights = F.softmax(self.weights, dim=1)\n",
    "        output = torch.matmul(x_sorted, normalized_weights.t())\n",
    "        if self.bias is not None:\n",
    "            output += self.bias\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE TO POOLING LAYER\n",
    "class ChoquetLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(ChoquetLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = bias\n",
    "\n",
    "        self.v = nn.Parameter(torch.empty(out_features, in_features))\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = nn.Parameter(torch.empty(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.init_parameters()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.in_features)\n",
    "        nn.init.uniform_(self.v, -stdv, stdv)\n",
    "        if self.use_bias:\n",
    "            nn.init.uniform_(self.bias, -stdv, stdv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() != 2:\n",
    "            raise ValueError(\"Input tensor must be 2D (batch_size, in_features)\")\n",
    "\n",
    "        x_sorted, _ = torch.sort(x, dim=1, descending=True)\n",
    "        v_padded = F.pad(self.v, (0, 1), \"constant\", 0)\n",
    "        delta_v = v_padded[:, :-1] - v_padded[:, 1:]\n",
    "        x_sorted_expanded = x_sorted.unsqueeze(1)  # (batch_size, 1, in_features)\n",
    "        delta_v_expanded = delta_v.unsqueeze(0)  # (1, out_features, in_features)\n",
    "        output = torch.sum(x_sorted_expanded * delta_v_expanded, dim=2)\n",
    "\n",
    "        if self.use_bias:\n",
    "            output += self.bias\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNetModel(nn.Module):\n",
    "\n",
    "    def __init__(self, conv_filters=[64, 64], linear_sizes=[384, 192], num_classes=10):\n",
    "        super(LeNetModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, conv_filters[0], [2,2], [1,1])\n",
    "        self.pool1 = nn.MaxPool2d([2,2], [2,2])\n",
    "        self.conv2 = nn.Conv2d(conv_filters[0], conv_filters[1], [2,2], [1,1])\n",
    "        self.pool2 = nn.MaxPool2d([2,2], [2,2])\n",
    "        self.fc1 = nn.Linear(conv_filters[1]*7*7, linear_sizes[0])       \n",
    "        self.fc2 = nn.Linear(linear_sizes[0], linear_sizes[1])\n",
    "        self.fc3 = nn.Linear(linear_sizes[1], num_classes)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir=RUNS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, val_loader=None, num_epochs=20, device='cuda'):\n",
    "    train_acc = []  \n",
    "    train_loss = []\n",
    "    if val_loader is not None:\n",
    "        val_acc = []\n",
    "        val_loss = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        count_evaluated = 0\n",
    "        count_correct = 0\n",
    "        \n",
    "        for batch_idx, data in enumerate(train_loader, 0):    \n",
    "            model.train()  \n",
    "            inputs, labels = data[0].to(device), data[1].to(device)  \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            count_evaluated += inputs.shape[0]\n",
    "            count_correct += torch.sum(labels == torch.max(outputs, dim=1)[1])\n",
    "            \n",
    "        print('Training: [%d, %5d] loss: %.3f' % (epoch + 1, batch_idx + 1, running_loss / (batch_idx+1)))\n",
    "        \n",
    "        train_loss.append(running_loss / (batch_idx+1))\n",
    "        train_acc.append(float(count_correct) / count_evaluated)\n",
    "        \n",
    "        if val_loader is not None:\n",
    "            running_loss_val = 0.0\n",
    "            count_evaluated = 0\n",
    "            count_correct = 0\n",
    "            model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for val_batch_idx, data_val in enumerate(val_loader, 0):\n",
    "                    inputs_val, labels_val = data_val[0].to(device), data_val[1].to(device)\n",
    "                    outputs_val = model(inputs_val)\n",
    "                    loss = criterion(outputs_val, labels_val)\n",
    "                    running_loss_val += loss.item()\n",
    "                    count_evaluated += inputs_val.shape[0]\n",
    "                    count_correct += torch.sum(labels_val == torch.max(outputs_val, dim=1)[1])\n",
    "                    \n",
    "                val_loss.append(running_loss_val / (val_batch_idx + 1))\n",
    "                acc_val = float(count_correct) / count_evaluated\n",
    "                \n",
    "                print('Validation: epoch %d - acc: %.3f' %\n",
    "                            (epoch + 1, acc_val))\n",
    "                val_acc.append(acc_val)\n",
    "                \n",
    "        # Tensorboard\n",
    "        writer.add_scalar('Loss/Validation', val_loss[-1], global_step=epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc[-1], global_step=epoch)\n",
    "        writer.add_scalar('Loss/Train', train_loss[-1], global_step=epoch)\n",
    "        writer.add_scalar('Accuracy/Train', train_acc[-1], global_step=epoch)\n",
    "        for name, param in model.named_parameters():\n",
    "            writer.add_histogram(f\"Parameters/{name}\", param, epoch)\n",
    "            if param.grad is not None:\n",
    "                writer.add_histogram(f\"Gradients/{name}\", param.grad, epoch)\n",
    "                \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, criterion, device='cuda'):\n",
    "    with torch.no_grad():\n",
    "        number_samples = 0\n",
    "        number_correct = 0\n",
    "        running_loss_test = 0.0\n",
    "        for test_batch_idx, data_test in enumerate(test_loader, 0):\n",
    "            inputs_test, labels_test = data_test[0].to(device), data_test[1].long().to(device)\n",
    "            outputs_test = model(inputs_test)\n",
    "            loss = criterion(outputs_test, labels_test)\n",
    "            running_loss_test += loss.cpu().numpy()\n",
    "            \n",
    "            _, outputs_class = torch.max(outputs_test, dim=1)\n",
    "            number_correct += torch.sum(outputs_class == labels_test).cpu().numpy()\n",
    "            number_samples += len(labels_test)\n",
    "            \n",
    "        acc_test = number_correct / number_samples\n",
    "        \n",
    "        print('Test - Accuracy: %.3f' % acc_test)\n",
    "        print('Test - CrossEntropy: %.3f' % (running_loss_test / (test_batch_idx+1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
